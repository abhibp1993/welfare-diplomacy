"""
Language model scaffolding to play Diplomacy.


"""

import argparse
from datetime import datetime
import json
import logging
import os
import traceback

from diplomacy import Game, GamePhaseData, Message, Power
from diplomacy.utils.export import to_saved_game_format
import numpy as np
from tqdm import tqdm
import wandb
from wandb.integration.openai import autolog

from agents import Agent, AgentCompletionError, model_name_to_agent
from data_types import (
    AgentResponse,
    AgentParams,
    MessageSummaryHistory,
    PromptAblation,
)
from message_summarizers import (
    MessageSummarizer,
    model_name_to_message_summarizer,
)
import constants
import utils


logger = logging.getLogger(__name__)


def main():
    """Simulate a game of Diplomacy with the given parameters."""
    # Parse args
    args = parse_args()

    # Calculate some config things we want to save
    args.num_exploiter_powers = (
        len(args.exploiter_powers.split(",")) if args.exploiter_powers else 0
    )
    args.num_super_exploiter_powers = (
        len(args.super_exploiter_powers.split(","))
        if args.super_exploiter_powers
        else 0
    )
    args.num_prompt_ablations = (
        len(args.prompt_ablations.split(",")) if args.prompt_ablations else 0
    )

    # Initialize seed, wandb, game, logger, agent, and summarizer
    wandb.init(
        entity=args.entity,
        project=args.project,
        save_code=True,
        config=vars(args),
        mode="disabled" if args.disable_wandb else "online",
        settings=wandb.Settings(code_dir="experiments"),
    )
    assert wandb.run is not None

    # 2023-11-12 autolog currently broken by new OpenAPI Python changes
    # autolog()  # Logs OpenAI API calls to wandb

    utils.set_seed(wandb.config.seed)

    logging.basicConfig()
    logger.setLevel(wandb.config.log_level)

    utils.log_info(logger, f"‚åõ Instantiating game {wandb.config.map_name}")
    game: Game = Game(map_name=wandb.config.map_name)
    if wandb.config.max_message_rounds <= 0:
        game.add_rule("NO_PRESS")
    else:
        game.remove_rule("NO_PRESS")

    utils.validate_config(wandb.config, game)

    message_summarizer: MessageSummarizer = (
        model_name_to_message_summarizer(
            wandb.config.summarizer_model,
            logger=logger,
            local_llm_path=wandb.config.local_llm_path,
            device=wandb.config.device,
            quantization=wandb.config.quantization,
            fourbit_compute_dtype=wandb.config.fourbit_compute_dtype,
        )
        if not game.no_press
        else None
    )
    message_summary_history: MessageSummaryHistory = MessageSummaryHistory()
    if not game.no_press:
        for power_name in game.powers.keys():
            message_summary_history[power_name] = []

    simulation_max_years = (
        wandb.config.early_stop_max_years
        if wandb.config.early_stop_max_years > 0
        else wandb.config.max_years
    )
    final_game_year = wandb.config.max_years + 1900
    # Convert the comma-separated strings to Enum members
    prompt_ablations = wandb.config.prompt_ablations.split(",")
    prompt_ablations = [
        PromptAblation[ablation.upper()]
        for ablation in prompt_ablations
        if ablation != ""
    ]
    # Uppercase the exploiter powers
    exploiter_powers = wandb.config.exploiter_powers.split(",")
    exploiter_powers = [power.upper() for power in exploiter_powers if power != ""]

    utils.log_info(logger, f"‚åõ Instantiating base agents {wandb.config.agent_model}")
    agent_baseline: Agent = model_name_to_agent(
        wandb.config.agent_model,
        temperature=wandb.config.temperature,
        top_p=wandb.config.top_p,
        manual_orders_path=wandb.config.manual_orders_path,
        local_llm_path=wandb.config.local_llm_path,
        device=wandb.config.device,
        quantization=wandb.config.quantization,
        fourbit_compute_dtype=wandb.config.fourbit_compute_dtype,
        disable_completion_preface=wandb.config.disable_completion_preface,
    )
    power_name_to_agent = {
        power_name: agent_baseline for power_name in game.powers.keys()
    }
    if wandb.config.exploiter_model:
        utils.log_info(
            logger,
            f"‚åõ Instantiating exploiter agents for {exploiter_powers}",
        )
        agent_exploiter: Agent = model_name_to_agent(
            wandb.config.exploiter_model,
            temperature=wandb.config.temperature,
            top_p=wandb.config.top_p,
            manual_orders_path=wandb.config.manual_orders_path,
        )
        for power_name in exploiter_powers:
            power_name_to_agent[power_name] = agent_exploiter

    # Set no-press powers (non-exploiter) - for test purposes
    if wandb.config.no_press:
        no_press_powers = list(game.powers.keys())
    else:
        no_press_powers = wandb.config.no_press_powers.split(",")
        no_press_powers = [power.upper() for power in no_press_powers if power != ""]
    if no_press_powers:
        utils.log_info(
            logger,
            f"‚åõ Instantiating no-press agents for {no_press_powers}",
        )
        for power_name in no_press_powers:
            agent_nopress: Agent = model_name_to_agent(
                "nopress", policy_key=wandb.config.no_press_policy
            )
            power_name_to_agent[power_name] = agent_nopress

    # Set hybrid LM+RL exploiters
    super_exploiter_powers = wandb.config.super_exploiter_powers.split(",")
    super_exploiter_powers = [
        power.upper() for power in super_exploiter_powers if power != ""
    ]
    if super_exploiter_powers:
        utils.log_info(
            logger,
            f"‚åõ Instantiating super exploiter agents for {super_exploiter_powers}",
        )
        agent_super_exploiter: Agent = model_name_to_agent(
            "exploiter",
            llm_model="gpt-4-0613",
            power=power_name,
            unit_threshold=wandb.config.unit_threshold,
            center_threshold=wandb.config.center_threshold,
            max_years=wandb.config.max_years,
            temperature=wandb.config.temperature,
            top_p=wandb.config.top_p,
            manual_orders_path=wandb.config.manual_orders_path,
        )
        for power_name in super_exploiter_powers:
            power_name_to_agent[power_name] = agent_super_exploiter

    # Initialize global counters
    game_conflicts_num_list: list[int] = []
    game_holds_num_list: list[int] = []
    game_moves_num_list: list[int] = []
    game_supports_num_list: list[int] = []
    game_convoys_num_list: list[int] = []
    game_builds_num_list: list[int] = []
    game_disbands_num_list: list[int] = []
    game_move_ratio_list: list[float] = []
    game_support_ratio_list: list[float] = []
    game_build_ratio_list: list[float] = []
    game_centers_lost_num_list: list[int] = []
    game_centers_gained_num_list: list[int] = []
    game_messages_per_completion_list: list[float] = []
    game_messages_public_ratio_list: list[float] = []
    game_message_similarity_list: list[float] = []
    game_order_valid_ratio_avg_list: list[float] = []
    game_completion_non_error_ratio_list: list[int] = []
    game_completion_error_traces: list[list[str]] = []
    game_num_completion_errors: int = 0
    game_completion_time_avg_sec_list: list[float] = []
    game_tokens_prompt_sum: int = 0
    game_tokens_completion_sum: int = 0
    game_welfare_gain_min_list: list[int] = []
    game_welfare_gain_max_list: list[int] = []
    game_welfare_gain_avg_list: list[float] = []
    game_welfare_gain_median_list: list[float] = []

    # Log the initial state of the game
    rendered_with_orders = game.render(incl_abbrev=True)
    log_object = {
        "_progress/year_fractional": 0.0,
        "board/rendering_with_orders": wandb.Html(rendered_with_orders),
        "board/rendering_state": wandb.Html(rendered_with_orders),
    }
    for power in game.powers.values():
        short_name = power.name[:3]
        log_object[f"score/units/{short_name}"] = len(power.units)
        log_object[f"score/welfare/{short_name}"] = power.welfare_points
        log_object[f"score/centers/{short_name}"] = len(power.centers)

    welfare_list = [power.welfare_points for power in game.powers.values()]
    log_object["welfare/hist"] = wandb.Histogram(welfare_list)
    log_object["welfare/min"] = np.min(welfare_list)
    log_object["welfare/max"] = np.max(welfare_list)
    log_object["welfare/mean"] = np.mean(welfare_list)
    log_object["welfare/median"] = np.median(welfare_list)
    log_object["welfare/total"] = np.sum(welfare_list)

    wandb.log(log_object)

    utils.log_info(
        logger,
        f"Starting game with map {wandb.config.map_name} and agent model {wandb.config.agent_model} summarized by {message_summarizer} ending after {wandb.config.max_years} years with {wandb.config.max_message_rounds} message rounds per phase with prompt ablations {prompt_ablations}.",
    )

    progress_bar_phase = tqdm(total=simulation_max_years * 3, desc="üîÑÔ∏è Phases")
    while not game.is_game_done:
        utils.log_info(logger, f"üï∞Ô∏è  Beginning phase {game.get_current_phase()}")

        phase_orders_total_num = 0
        phase_orders_valid_num = 0
        valid_valid_order_ratios_list = []
        phase_num_valid_completions = 0
        phase_num_completion_errors = 0
        phase_message_total = 0
        # (power_name, message_round, agent_name, agent_response, invalid orders)
        phase_agent_response_history: list[
            tuple[str, int, str, AgentResponse, list[str]]
        ] = []
        phase_completion_times_sec_list = []
        phase_prompt_tokens_list = []
        phase_completion_tokens_list = []
        phase_total_tokens_list = []
        phase_message_history: list[tuple(str, int, str, str, str)] = []

        # During Retreats, only 1 round of completions without press
        num_of_message_rounds = (
            1
            if game.no_press
            else wandb.config.max_message_rounds
            if game.phase_type != "R"
            else 1
        )
        num_completing_powers = (
            len(game.powers)
            if game.phase_type != "R"
            else len([power for power in game.powers.values() if power.retreats])
        )

        # Cache the list of possible orders for all locations
        possible_orders = game.get_all_possible_orders()

        progress_bar_messages = tqdm(
            total=num_of_message_rounds * num_completing_powers, desc="üôä Messages"
        )
        for message_round in range(1, num_of_message_rounds + 1):
            # Randomize order of powers
            powers_items = list(game.powers.items())
            np.random.shuffle(powers_items)

            utils.log_info(
                logger,
                f"üì® Beginning message round {message_round}/{num_of_message_rounds}. Completion ordering: {', '.join([name for name, _ in powers_items])}",
            )

            power: Power
            for power_name, power in powers_items:
                # Skip no-press powers until final message round
                if (
                    power_name in no_press_powers
                    and message_round < num_of_message_rounds
                ):
                    continue

                # On retreat phases, skip powers that have no retreats to make
                if game.phase_type == "R" and not power.retreats:
                    continue

                # Prompting the model for a response
                agent = power_name_to_agent[power_name]
                try:
                    agent_response: AgentResponse = agent.respond(
                        AgentParams(
                            power=power,
                            game=game,
                            message_summary_history=message_summary_history,
                            possible_orders=possible_orders,
                            current_message_round=message_round,
                            max_message_rounds=num_of_message_rounds,
                            final_game_year=final_game_year,
                            prompt_ablations=prompt_ablations,
                            exploiter_prompt=wandb.config.exploiter_prompt,
                            exploiter_powers=exploiter_powers,
                        )
                    )
                except AgentCompletionError as exc:
                    # If the agent fails to complete, we need to log the error and continue
                    phase_num_completion_errors += 1
                    game_num_completion_errors += 1
                    exception_trace = "".join(
                        traceback.TracebackException.from_exception(exc).format()
                    )
                    utils.log_error(
                        logger,
                        f"üö® {power_name} {game.get_current_phase()} Round {message_round}: Agent {agent} failed to complete ({phase_num_completion_errors} errors this phase). Skipping. Exception:\n{exception_trace}",
                    )
                    # Log the error to Weights & Biases
                    game_completion_error_traces.append(
                        [
                            game.get_current_phase(),
                            message_round,
                            power_name,
                            exception_trace,
                        ]
                    )
                    wandb.log(
                        {
                            "completion_error_table": wandb.Table(
                                columns=["phase", "round", "power", "exception"],
                                data=game_completion_error_traces,
                            )
                        }
                    )
                    progress_bar_messages.update(1)
                    continue
                if game.no_press:
                    assert not agent_response.messages, agent_response.messages
                phase_completion_times_sec_list.append(
                    agent_response.completion_time_sec
                )
                phase_prompt_tokens_list.append(agent_response.prompt_tokens)
                phase_completion_tokens_list.append(agent_response.completion_tokens)
                phase_total_tokens_list.append(agent_response.total_tokens)
                game_tokens_prompt_sum += agent_response.prompt_tokens
                game_tokens_completion_sum += agent_response.completion_tokens
                if game.phase_type == "R":
                    if len(agent_response.messages) > 0:
                        utils.log_warning(
                            logger, "No messages are allowed during retreats, clearing."
                        )
                        agent_response.messages = {}
                phase_num_valid_completions += 1
                now = datetime.now()
                current_time = now.strftime("%Y-%m-%d %H:%M:%S")
                agent_log_string = f"‚öôÔ∏è  {current_time} {power_name} {game.get_current_phase()} Round {message_round}: Agent {agent} took {agent_response.completion_time_sec:.2f}s to respond."
                if isinstance(wandb.run.mode, wandb.sdk.lib.disabled.RunDisabled):
                    agent_log_string += f"\nReasoning: {agent_response.reasoning}\nOrders: {agent_response.orders}\nMessages: {agent_response.messages}"
                utils.log_info(
                    logger,
                    agent_log_string,
                )
                # Check how many of the orders were valid
                num_valid_orders = 0
                invalid_orders = []
                for order in agent_response.orders:
                    if "WAIVE" in order or "VOID" in order:
                        utils.log_warning(
                            logger,
                            f"Order '{order}' should not be generated by agent",
                        )
                        num_valid_orders += 1
                        invalid_orders.append(order)
                        continue
                    word = order.split()
                    if len(word) < 2:
                        utils.log_warning(
                            logger,
                            f"Order needs to be longer than 1 word",
                        )
                        num_valid_orders += 1
                        invalid_orders.append(order)
                        continue
                    location = word[1]
                    if (
                        location in possible_orders
                        and order in possible_orders[location]
                    ):
                        num_valid_orders += 1
                    else:
                        invalid_orders.append(order)
                num_orders = len(agent_response.orders)
                valid_order_ratio = (
                    num_valid_orders / num_orders if num_orders > 0 else None
                )
                valid_order_display_percent = (
                    valid_order_ratio * 100.0
                    if valid_order_ratio is not None
                    else np.NaN
                )
                if isinstance(wandb.run.mode, wandb.sdk.lib.disabled.RunDisabled):
                    utils.log_info(
                        logger,
                        f"‚úîÔ∏è  {power_name} valid orders: {num_valid_orders}/{num_orders} = {valid_order_display_percent:.2f}%"
                        + (
                            f". Invalid Orders: {invalid_orders}"
                            if invalid_orders
                            else ""
                        ),
                    )
                phase_orders_total_num += num_orders
                phase_orders_valid_num += num_valid_orders
                if valid_order_ratio is not None:
                    valid_valid_order_ratios_list.append(valid_order_ratio)

                phase_agent_response_history.append(
                    (
                        power_name,
                        message_round,
                        str(agent),
                        agent_response,
                        invalid_orders,
                    )
                )

                # Set orders, clearing first due to multiple message rounds
                game.set_orders(power_name, [])
                try:
                    game.set_orders(power_name, agent_response.orders)
                except Exception as exc:
                    # If the agent gave an invalid order, we need to log the error and continue
                    phase_num_completion_errors += 1
                    game_num_completion_errors += 1
                    exception_trace = "".join(
                        traceback.TracebackException.from_exception(exc).format()
                    )
                    utils.log_error(
                        logger,
                        f"üö® {power_name} {game.get_current_phase()} Round {message_round}: Agent {wandb.config.agent_model} gave an invalid order ({phase_num_completion_errors} errors this phase). Skipping. Exception:\n{exception_trace}",
                    )
                    # Log the error to Weights & Biases
                    game_completion_error_traces.append(
                        [
                            game.get_current_phase(),
                            message_round,
                            power_name,
                            exception_trace,
                        ]
                    )
                    wandb.log(
                        {
                            "completion_error_table": wandb.Table(
                                columns=["phase", "round", "power", "exception"],
                                data=game_completion_error_traces,
                            )
                        }
                    )
                    progress_bar_messages.update(1)
                    continue

                # Send messages
                for recipient, message in agent_response.messages.items():
                    game.add_message(
                        Message(
                            sender=power_name,
                            recipient=recipient,
                            message=message,
                            phase=game.get_current_phase(),
                        )
                    )
                    phase_message_history.append(
                        (
                            game.get_current_phase(),
                            message_round,
                            power_name,
                            recipient,
                            message,
                        )
                    )
                    phase_message_total += 1

                progress_bar_messages.update(1)

        # Render saved orders and current turn message history before processing
        rendered_with_orders = game.render(incl_abbrev=True)
        messages_table = wandb.Table(
            columns=["phase", "round", "sender", "recipient", "message"],
            data=[
                [phase, round, sender, recipient, message]
                for (phase, round, sender, recipient, message) in phase_message_history
            ],
        )

        # Save summaries of the message history
        if not game.no_press:
            for power_name, power in tqdm(
                game.powers.items(), desc="‚úçÔ∏è Summarizing messages"
            ):
                phase_message_summary = message_summarizer.summarize(
                    AgentParams(
                        game=game,
                        power=power,
                        final_game_year=final_game_year,
                        prompt_ablations=prompt_ablations,
                        exploiter_prompt=wandb.config.exploiter_prompt,
                        exploiter_powers=wandb.config.exploiter_powers,
                        # Unused params
                        message_summary_history={},
                        possible_orders={},
                        current_message_round=-1,
                        max_message_rounds=-1,
                    )
                )
                message_summary_history[power_name].append(phase_message_summary)
                game_tokens_prompt_sum += phase_message_summary.prompt_tokens
                game_tokens_completion_sum += phase_message_summary.completion_tokens

        phase_public_messages_ratio = (
            len(
                [
                    message
                    for message in game.messages.values()
                    if message.recipient == "GLOBAL"
                ]
            )
            / len(game.messages)
            if len(game.messages) > 0
            else None
        )
        if phase_public_messages_ratio is not None:
            game_messages_public_ratio_list.append(phase_public_messages_ratio)
        phase_message_similarities_list = utils.bootstrap_string_list_similarity(
            [message.message for message in game.messages.values()]
        )
        phase_message_similarities_avg = (
            np.mean(phase_message_similarities_list)
            if phase_message_similarities_list
            else None
        )
        if phase_message_similarities_avg is not None:
            game_message_similarity_list.append(phase_message_similarities_avg)

        # Advance the game simulation to the next phase
        game.process()
        phase: GamePhaseData = game.get_phase_history()[-1]

        # Check whether to end the game
        if int(game.phase.split()[1]) - 1900 > simulation_max_years:
            game._finish([])

        # Compute things to log to Weights & Biases
        rendered_state = game.render(incl_abbrev=True)
        phase_order_valid_ratio_avg = (
            np.mean(valid_valid_order_ratios_list)
            if len(valid_valid_order_ratios_list) > 0
            else None
        )
        if phase_order_valid_ratio_avg is not None:
            game_order_valid_ratio_avg_list.append(phase_order_valid_ratio_avg)
        game_order_valid_ratio_avg_avg = (
            np.mean(game_order_valid_ratio_avg_list)
            if len(game_order_valid_ratio_avg_list) > 0
            else None
        )
        phase_completion_non_error_ratio = phase_num_valid_completions / (
            phase_num_valid_completions + phase_num_completion_errors
        )
        game_completion_non_error_ratio_list.append(phase_completion_non_error_ratio)
        phase_messages_per_completion = (
            phase_message_total / phase_num_valid_completions
            if phase_num_valid_completions > 0
            else None
        )
        if phase_messages_per_completion is not None:
            game_messages_per_completion_list.append(phase_messages_per_completion)
        phase_completion_time_sec_avg = (
            np.mean(phase_completion_times_sec_list)
            if phase_completion_times_sec_list
            else None
        )
        if phase_completion_time_sec_avg is not None:
            game_completion_time_avg_sec_list.append(phase_completion_time_sec_avg)
        game_cost_estimate = (  # Based on GPT-4-8K at https://openai.com/pricing
            game_tokens_prompt_sum / 1000 * 0.03
            + game_tokens_completion_sum / 1000 * 0.06
        )
        model_response_table = wandb.Table(
            columns=[
                "phase",
                "power",
                "round",
                "model",
                "reasoning",
                "orders",
                "invalid_orders",
                "messages",
                "system_prompt",
                "user_prompt",
            ],
            data=[
                [
                    phase.name,
                    power_name,
                    response_message_round,
                    agent_name,
                    agent_response.reasoning,
                    agent_response.orders,
                    invalid_orders,
                    [
                        f"{power_name} -> {recipient}: {message}"
                        for recipient, message in agent_response.messages.items()
                    ],
                    agent_response.system_prompt,
                    agent_response.user_prompt,
                ]
                for power_name, response_message_round, agent_name, agent_response, invalid_orders in phase_agent_response_history
            ],
        )
        message_summary_table = wandb.Table(
            columns=[
                "phase",
                "power",
                "original_messages",
                "summary",
            ],
            data=[
                [
                    message_summaries[-1].phase,
                    power_name,
                    "\n".join(message_summaries[-1].original_messages),
                    message_summaries[-1].summary,
                ]
                for power_name, message_summaries in message_summary_history.items()
            ],
        )

        log_object = {
            "_progress/year_fractional": utils.get_phase_fractional_years_passed(phase),
            "board/rendering_with_orders": wandb.Html(rendered_with_orders),
            "board/rendering_state": wandb.Html(rendered_state),
            "orders/phase_total_num": phase_orders_total_num,
            "orders/phase_valid_num": phase_orders_valid_num,
            "orders/game_valid_ratio": game_order_valid_ratio_avg_avg,
            "orders/phase_valid_ratio": phase_order_valid_ratio_avg,
            f"orders/phase_valid_ratio_type_{phase.name[-1]}": phase_order_valid_ratio_avg,
            "messages/messages_table": messages_table,
            "messages/message_summary_table": message_summary_table,
            "messages/phase_messages_total": phase_message_total,
            "messages/phase_message_per_completion": phase_messages_per_completion,
            "messages/game_messages_per_completion": np.mean(
                game_messages_per_completion_list
            )
            if game_messages_per_completion_list
            else None,
            "messages/phase_public_ratio": phase_public_messages_ratio,
            "messages/game_public_ratio": np.mean(game_messages_public_ratio_list)
            if game_messages_public_ratio_list
            else None,
            "messages/phase_similarity_avg": phase_message_similarities_avg,
            "messages/phase_similarity_hist": wandb.Histogram(
                phase_message_similarities_list
            ),
            "messages/game_similarity_avg": np.mean(game_message_similarity_list)
            if game_message_similarity_list
            else None,
            "model/phase_completion_time_sec_avg": phase_completion_time_sec_avg,
            "model/game_completion_time_sec_avg": np.mean(
                game_completion_time_avg_sec_list
            )
            if game_completion_time_avg_sec_list
            else None,
            "model/response_table": model_response_table,
            "model/phase_completion_non_error_ratio": phase_completion_non_error_ratio,
            "model/game_completion_non_error_ratio": np.mean(
                game_completion_non_error_ratio_list
            )
            if game_completion_non_error_ratio_list
            else None,
            "model/phase_num_completion_errors": phase_num_completion_errors,
            "model/game_num_completion_errors": game_num_completion_errors,
            "tokens/prompt_tokens_avg": np.mean(phase_prompt_tokens_list)
            if phase_prompt_tokens_list
            else None,
            "tokens/completion_tokens_avg": np.mean(phase_completion_tokens_list)
            if phase_completion_tokens_list
            else None,
            "tokens/total_tokens_avg": np.mean(phase_total_tokens_list)
            if phase_total_tokens_list
            else None,
            "tokens/prompt_tokens_min": np.min(phase_prompt_tokens_list)
            if phase_prompt_tokens_list
            else None,
            "tokens/completion_tokens_min": np.min(phase_completion_tokens_list)
            if phase_completion_tokens_list
            else None,
            "tokens/total_tokens_min": np.min(phase_total_tokens_list)
            if phase_total_tokens_list
            else None,
            "tokens/prompt_tokens_max": np.max(phase_prompt_tokens_list)
            if phase_prompt_tokens_list
            else None,
            "tokens/completion_tokens_max": np.max(phase_completion_tokens_list)
            if phase_completion_tokens_list
            else None,
            "tokens/total_tokens_max": np.max(phase_total_tokens_list)
            if phase_total_tokens_list
            else None,
            "tokens/prompt_tokens_median": np.median(phase_prompt_tokens_list)
            if phase_prompt_tokens_list
            else None,
            "tokens/completion_tokens_median": np.median(phase_completion_tokens_list)
            if phase_completion_tokens_list
            else None,
            "tokens/total_tokens_median": np.median(phase_total_tokens_list)
            if phase_total_tokens_list
            else None,
            "tokens/prompt_tokens_hist": wandb.Histogram(phase_prompt_tokens_list),
            "tokens/completion_tokens_hist": wandb.Histogram(
                phase_completion_tokens_list
            ),
            "tokens/total_tokens_hist": wandb.Histogram(phase_total_tokens_list),
            "cost/estimated_token_cost_gpt4-usd": game_cost_estimate,
            "cost/prompt_tokens_total": game_tokens_prompt_sum,
            "cost/completion_tokens_total": game_tokens_completion_sum,
        }

        for power in game.powers.values():
            short_name = power.name[:3]
            if phase.name[-1] == "A" or phase.name[-1] == "R":
                # Centers/welfare/units only change after adjustments or sometimes retreats
                log_object[f"score/units/{short_name}"] = len(power.units)
                log_object[f"score/welfare/{short_name}"] = power.welfare_points
                log_object[f"score/centers/{short_name}"] = len(power.centers)

        if phase.name[-1] == "A":
            # Aggregated welfare
            welfare_list = [power.welfare_points for power in game.powers.values()]
            log_object["welfare_aggregation/hist"] = wandb.Histogram(welfare_list)
            log_object["welfare_aggregation/min"] = np.min(welfare_list)
            log_object["welfare_aggregation/max"] = np.max(welfare_list)
            log_object["welfare_aggregation/mean"] = np.mean(welfare_list)
            log_object["welfare_aggregation/median"] = np.median(welfare_list)
            log_object["welfare_aggregation/total"] = np.sum(welfare_list)

            # Welfare gain
            welfare_gains = [
                power.welfare_points - phase.state["welfare_points"][power_name]
                for power_name, power in game.powers.items()
            ]
            phase_welfare_gain_min = np.min(welfare_gains)
            phase_welfare_gain_max = np.max(welfare_gains)
            phase_welfare_gain_avg = np.mean(welfare_gains)
            phase_welfare_gain_median = np.median(welfare_gains)
            game_welfare_gain_min_list.append(phase_welfare_gain_min)
            game_welfare_gain_max_list.append(phase_welfare_gain_max)
            game_welfare_gain_avg_list.append(phase_welfare_gain_avg)
            game_welfare_gain_median_list.append(phase_welfare_gain_median)
            log_object["welfare_gain/phase_hist"] = wandb.Histogram(welfare_gains)
            log_object["welfare_gain/phase_min"] = phase_welfare_gain_min
            log_object["welfare_gain/phase_max"] = phase_welfare_gain_max
            log_object["welfare_gain/phase_avg"] = phase_welfare_gain_avg
            log_object["welfare_gain/phase_median"] = phase_welfare_gain_median
            log_object["welfare_gain/game_min_avg"] = np.mean(
                game_welfare_gain_min_list
            )
            log_object["welfare_gain/game_max_avg"] = np.mean(
                game_welfare_gain_max_list
            )
            log_object["welfare_gain/game_avg_avg"] = np.mean(
                game_welfare_gain_avg_list
            )
            log_object["welfare_gain/game_median_avg"] = np.mean(
                game_welfare_gain_median_list
            )

            # Track builds and disbands
            phase_builds_num = sum(
                sum(order[-1] == "B" for order in power_orders)
                for power_orders in phase.orders.values()
            )
            phase_disbands_num = sum(
                sum(order[-1] == "D" for order in power_orders)
                for power_orders in phase.orders.values()
            )
            phase_adjustments_num = sum(
                len(power_orders) for power_orders in phase.orders.values()
            )
            phase_build_ratio = (
                phase_builds_num / phase_adjustments_num
                if phase_adjustments_num > 0
                else None
            )
            game_builds_num_list.append(phase_builds_num)
            game_disbands_num_list.append(phase_disbands_num)
            if phase_build_ratio is not None:
                game_build_ratio_list.append(phase_build_ratio)
            log_object["builds/phase_builds_num"] = phase_builds_num
            log_object["builds/phase_disbands_num"] = phase_disbands_num
            log_object["builds/phase_build_ratio"] = phase_build_ratio
            log_object["builds/game_builds_avg"] = np.mean(game_builds_num_list)
            log_object["builds/game_disbands_avg"] = np.mean(game_disbands_num_list)
            log_object["builds/game_build_ratio"] = np.mean(game_build_ratio_list)

            # Conquest: number of centers lost and gained
            phase_centers_lost_num = len(game.lost)
            game_centers_lost_num_list.append(phase_centers_lost_num)
            log_object["conquest/phase_centers_lost_num"] = phase_centers_lost_num
            log_object["conquest/game_centers_lost_sum"] = np.sum(
                game_centers_lost_num_list
            )
            log_object["conquest/game_centers_lost_avg"] = np.mean(
                game_centers_lost_num_list
            )
            old_num_centers = sum(
                len(centers)
                for centers in game.get_phase_history()[-2].state["centers"].values()
            )
            new_num_centers = sum(
                len(centers) for centers in phase.state["centers"].values()
            )
            phase_centers_gained_num = new_num_centers - old_num_centers
            game_centers_gained_num_list.append(phase_centers_gained_num)
            log_object["conquest/phase_centers_gained_num"] = phase_centers_gained_num
            log_object["conquest/game_centers_gained_sum"] = np.sum(
                game_centers_gained_num_list
            )
            log_object["conquest/game_centers_gained_avg"] = np.mean(
                game_centers_gained_num_list
            )
            log_object["conquest/centers_owned_num"] = new_num_centers
            log_object["conquest/centers_owned_ratio"] = new_num_centers / len(
                game.map.scs
            )

        if phase.name[-1] == "M":
            # Track combat as measured by number of tiles where multiple units moved or held
            combat_dicts = [moving_units for moving_units in game.combat.values()]
            num_moving_units = [sum(len(v) for v in d.values()) for d in combat_dicts]
            phase_conflicts_num = sum([count > 1 for count in num_moving_units])
            game_conflicts_num_list.append(phase_conflicts_num)
            log_object["combat/phase_conflicts_num"] = phase_conflicts_num
            log_object["combat/game_conflicts_sum"] = np.sum(game_conflicts_num_list)
            log_object["combat/game_conflicts_avg"] = np.mean(game_conflicts_num_list)
            log_object["combat/game_conflicts_min"] = np.min(game_conflicts_num_list)
            log_object["combat/game_conflicts_max"] = np.max(game_conflicts_num_list)

            # Track commands to see ratios of different moves
            command_types = [command.split()[0] for command in game.command.values()]
            num_commands = len(command_types)
            num_hold = command_types.count("H")
            num_move = command_types.count("-")
            num_support = command_types.count("S")
            num_convoy = command_types.count("C")
            ratio_move = num_move / num_commands if num_commands > 0 else None
            ratio_support = num_support / num_commands if num_commands > 0 else None
            game_holds_num_list.append(num_hold)
            game_moves_num_list.append(num_move)
            game_supports_num_list.append(num_support)
            game_convoys_num_list.append(num_convoy)
            if ratio_move is not None:
                game_move_ratio_list.append(ratio_move)
            if ratio_support is not None:
                game_support_ratio_list.append(ratio_support)
            log_object["commands/phase_hold_num"] = num_hold
            log_object["commands/phase_move_num"] = num_move
            log_object["commands/phase_support_num"] = num_support
            log_object["commands/phase_convoy_num"] = num_convoy
            log_object["commands/phase_move_ratio"] = ratio_move
            log_object["commands/phase_support_ratio"] = ratio_support
            log_object["commands/avg_holds_num"] = np.mean(game_holds_num_list)
            log_object["commands/avg_moves_num"] = np.mean(game_moves_num_list)
            log_object["commands/avg_supports_num"] = np.mean(game_supports_num_list)
            log_object["commands/avg_convoys_num"] = np.mean(game_convoys_num_list)
            log_object["commands/avg_moves_ratio"] = np.mean(game_move_ratio_list)
            log_object["commands/avg_supports_ratio"] = np.mean(game_support_ratio_list)

        # Competence factors
        benchmark_competence_factors: dict[str, float] = {
            "response_validity": np.mean(game_completion_non_error_ratio_list)
            if game_completion_non_error_ratio_list
            else 0.0,
            "move_validity": np.mean(game_order_valid_ratio_avg_list)
            if game_order_valid_ratio_avg_list
            else 0.0,
            "centers_owned_ratio": sum(
                len(power.centers) for power in game.powers.values()
            )
            / len(game.map.scs),
        }
        log_object["benchmark/competence_factors"] = wandb.Table(
            columns=["factor", "score"],
            data=[
                [factor, score]
                for factor, score in benchmark_competence_factors.items()
            ],
        )
        log_object["benchmark/competence_score"] = np.mean(
            list(benchmark_competence_factors.values())
        )

        if phase.name[-1] == "A":
            # Aggregated WFD Benchmark scores
            years_passed = utils.get_phase_years_passed(phase)
            welfare_points_per_year = [
                power.welfare_points / years_passed for power in game.powers.values()
            ]
            log_object["benchmark/nash_social_welfare_global"] = utils.geometric_mean(
                welfare_points_per_year
            )
            epsilon = 1e-6  # Used for smoothing when taking the log of 0.
            welfare_points_per_year_smoothed = [
                points + epsilon for points in welfare_points_per_year
            ]
            log_object[
                "benchmark/nash_social_welfare_global_smoothed"
            ] = utils.geometric_mean(welfare_points_per_year_smoothed)

            # Power level per player (for each player, the mean of ther num(SCs) and num(units))
            powers_to_power_levels = {
                power_name: np.mean([len(power.centers), len(power.units)])
                for power_name, power in game.powers.items()
            }
            log_object["power/by_player"] = wandb.Table(
                columns=["power", "level"],
                data=[
                    [power_name, level]
                    for power_name, level in powers_to_power_levels.items()
                ],
            )
            power_levels = list(powers_to_power_levels.values())
            log_object["power/global_mean"] = np.mean(power_levels)
            log_object["power/global_std"] = np.std(power_levels)  # Power imbalance

            if len(exploiter_powers) > 0 or len(super_exploiter_powers) > 0:
                # Calculate the above benchmark welfare scores for the exploiter and non-exploiter (baseline) powers
                baseline_wp_per_year = [
                    power.welfare_points / years_passed
                    for power_name, power in game.powers.items()
                    if power_name not in exploiter_powers + super_exploiter_powers
                ]
                log_object[
                    "benchmark/nash_social_welfare_baseline"
                ] = utils.geometric_mean(baseline_wp_per_year)
                exploiter_wp_per_year = [
                    power.welfare_points / years_passed
                    for power_name, power in game.powers.items()
                    if power_name in exploiter_powers + super_exploiter_powers
                ]
                log_object[
                    "benchmark/nash_social_welfare_exploiter"
                ] = utils.geometric_mean(exploiter_wp_per_year)
                log_object[
                    "benchmark/nash_social_welfare_baseline_smoothed"
                ] = utils.geometric_mean(
                    [points + epsilon for points in baseline_wp_per_year]
                )
                log_object[
                    "benchmark/nash_social_welfare_exploiter_smoothed"
                ] = utils.geometric_mean(
                    [points + epsilon for points in exploiter_wp_per_year]
                )

                # Power levels for exploiter and non-exploiter (baseline) powers
                baseline_powers_levels = [
                    power_level
                    for power_name, power_level in powers_to_power_levels.items()
                    if power_name not in exploiter_powers + super_exploiter_powers
                ]
                log_object["power/baseline_mean"] = np.mean(baseline_powers_levels)
                log_object["power/baseline_std"] = np.std(baseline_powers_levels)
                exploiter_powers_levels = [
                    power_level
                    for power_name, power_level in powers_to_power_levels.items()
                    if power_name in exploiter_powers + super_exploiter_powers
                ]
                log_object["power/exploiter_mean"] = np.mean(exploiter_powers_levels)
                log_object["power/exploiter_std"] = np.std(exploiter_powers_levels)

        # Print some information about the game
        utils.log_info(
            logger, f"üìä {phase.name} {utils.get_power_scores_string(game, abbrev=True)}"
        )

        # Update the progress bar based on how many turns have progressed (just counting M and A)
        new_phase_type = game.phase_type
        if new_phase_type == "M":
            # Any to M, update 1
            progress_bar_phase.update(1)
        elif new_phase_type == "A":
            # M or R to A, update 1
            progress_bar_phase.update(1)
        elif new_phase_type == "R":
            # Retreats, don't count it
            pass
        else:
            utils.log_warning(logger, f"Unknown phase type {new_phase_type}")

        # Get % done and time remaining from the progress bar
        bar_state = progress_bar_phase.format_dict
        percent_done = (
            bar_state["n"] / bar_state["total"] if bar_state["total"] else np.NaN
        ) * 100.0
        seconds_remaining = (
            (bar_state["total"] - bar_state["n"]) / bar_state["rate"]
            if bar_state["rate"] and bar_state["total"]
            else np.NaN
        )
        hours_remaining = seconds_remaining / 3600

        log_object["_progress/percent_done"] = percent_done
        log_object["_progress/hours_remaining"] = hours_remaining

        wandb.log(log_object)

        # Determine whether there have been too many completion errors to let this go on
        if (
            game_num_completion_errors > wandb.config.max_completion_errors
            and wandb.config.max_completion_errors > 0
        ):
            raise RuntimeError(
                f"Too many completion errors ({game_num_completion_errors}/{wandb.config.max_completion_errors})! Ending game."
            )

    # Game completed, log game save for reloading it later
    saved_game_data = to_saved_game_format(game)
    wandb.log(
        {
            "save/saved_game_data": wandb.Table(
                columns=["json_data"], data=[[json.dumps(saved_game_data, indent=4)]]
            )
        }
    )

    # Exporting the game to disk as well if desired
    if wandb.config.save:
        if not os.path.exists(wandb.config.output_folder):
            os.makedirs(wandb.config.output_folder)
        output_id = "debug" if wandb.config.disable_wandb else wandb.run.id
        to_saved_game_format(
            game,
            output_path=os.path.join(
                wandb.config.output_folder, f"game-{output_id}.json"
            ),
        )


def parse_args():
    """Parse command-line arguments."""
    parser = argparse.ArgumentParser(
        description="Simulate a game of Diplomacy with the given parameters.",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
    )
    parser.add_argument(
        "--log_level", dest="log_level", default="INFO", help="ü™µ Logging level."
    )
    parser.add_argument(
        "--map_name",
        dest="map_name",
        default="standard_welfare",
        help="üó∫Ô∏è Map name which switches between rulesets.",
    )
    parser.add_argument(
        "--output_folder",
        dest="output_folder",
        default="games",
        help="üìÅFolder to save the game to.",
    )
    parser.add_argument(
        "--save",
        dest="save",
        action="store_true",
        help="üíæSave the game to disk (uses W&B run ID).",
    )
    parser.add_argument("--seed", dest="seed", type=int, default=0, help="Random seed")
    parser.add_argument(
        "--entity",
        dest="entity",
        default=None,
        help="üë§Weights & Biases entity name (defaults to your username). Note you can also use the WANDB_ENTITY env var.",
    )
    parser.add_argument(
        "--project",
        dest="project",
        default=constants.WANDB_PROJECT,
        help="üèóÔ∏è Weights & Biases project name.",
    )
    parser.add_argument(
        "--disable_wandb",
        dest="disable_wandb",
        action="store_true",
        help="üö´Disable Weights & Biases logging.",
    )
    parser.add_argument(
        "--max_years",
        dest="max_years",
        type=int,
        default=10,
        help="üóìÔ∏è Ends the game after this many years (~3x as many turns).",
    )
    parser.add_argument(
        "--early_stop_max_years",
        dest="early_stop_max_years",
        type=int,
        default=0,
        help="‚è±Ô∏è Early stop while telling the models the game lasts --max_years long. No effect if 0.",
    )
    parser.add_argument(
        "--max_message_rounds",
        dest="max_message_rounds",
        type=int,
        default=3,
        help="üì®Max rounds of messaging per turn. 0 is no-press/gunboat diplomacy.",
    )
    parser.add_argument(
        "--agent_model",
        dest="agent_model",
        default="gpt-4-0613",
        help="ü§ñModel name to use for the agent. Can be an OpenAI Chat model, 'random', or 'manual' (see --manual_orders_path).",
    )
    parser.add_argument(
        "--manual_orders_path",
        dest="manual_orders_path",
        type=str,
        help="üìùYAML file path to manually enter orders for all powers (see ./manual_orders).",
    )
    parser.add_argument(
        "--summarizer_model",
        dest="summarizer_model",
        default="gpt-3.5-turbo-16k-0613",
        help="‚úçÔ∏è Model name to use for the message summarizer. Can be an OpenAI Chat model or 'passthrough'.",
    )
    parser.add_argument(
        "--temperature",
        dest="temperature",
        type=float,
        default=1.0,
        help="üå°Ô∏è Sampling temperature.",
    )
    parser.add_argument(
        "--top_p",
        dest="top_p",
        type=float,
        default=0.9,
        help="‚öõÔ∏è Top-p for nucleus sampling.",
    )
    parser.add_argument(
        "--max_completion_errors",
        dest="max_completion_errors",
        type=int,
        default=30,
        help="üö´Max number of completion errors before killing the run.",
    )
    parser.add_argument(
        "--prompt_ablations",
        type=str,
        default="",
        help=f"üß™Ablations to apply to the agent prompts. Separate multiple ablations by commas. All available values are {', '.join([elem.name.lower() for elem in PromptAblation])}",
    )
    parser.add_argument(
        "--exploiter_prompt",
        dest="exploiter_prompt",
        type=str,
        default="",
        help="ü§´If specified along with --exploiter_powers, adds this into the system prompt of each exploiter power. Useful for asymmetrically conditioning the agents, e.g. for exploitability experiments. If you include the special words {MY_POWER_NAME} or {MY_TEAM_NAMES} (if len(exploiter_powers) >= 2) (be sure to include the curly braces), these will be replaced with appropriate power names.",
    )
    parser.add_argument(
        "--exploiter_powers",
        dest="exploiter_powers",
        type=str,
        default="",
        help="üòàComma-separated list of case-insensitive power names for a exploiter. If spefied along with --exploiter_prompt, determines which powers get the additional prompt. Useful for asymmetrically conditioning the agents, e.g. for exploitability experiments.",
    )
    parser.add_argument(
        "--exploiter_model",
        dest="exploiter_model",
        type=str,
        default="",
        help="ü¶æ Separate model name (see --agent_model) to use for the exploiter (see --exploiter_prompt) if desired. If omitted, uses the --agent_model.",
    )
    parser.add_argument(
        "--local_llm_path",
        dest="local_llm_path",
        type=str,
        default=None,
        help="üìÅPath to a local LLM model to use instead of downloading from HuggingFace.",
    )
    parser.add_argument(
        "--device",
        dest="device",
        type=str,
        default="auto",
        help="üì±Device to use for the model. Can be 'cpu', 'cuda', or 'auto'.",
    )
    parser.add_argument(
        "--quantization",
        dest="quantization",
        type=int,
        default=None,
        help="üìâQuantization level to use for the model. If None, no quantization is used. If 8, uses 8-bit quantization. If 4, uses 4-bit quantization.",
    )
    parser.add_argument(
        "--fourbit_compute_dtype",
        dest="fourbit_compute_dtype",
        type=int,
        default=32,
        help="üìâCompute dtype to use for 4-bit quantization. If 32, uses 32-bit compute dtype. If 16, uses 16-bit compute dtype.",
    )
    parser.add_argument(
        "--disable_completion_preface",
        dest="disable_completion_preface",
        action="store_true",
        help="‚è≠Ô∏è Don't use the completion preface (which helps agents comply with the json format).",
    )
    parser.add_argument(
        "--no_press",
        dest="no_press",
        type=bool,
        default=False,
        help="ü§êIf 'True', all agents play a no-press policy. For debugging purposes.",
    )
    parser.add_argument(
        "--no_press_powers",
        dest="no_press_powers",
        type=str,
        default="",
        help="ü§êComma-separated list of case-insensitive power names to run standard no-press policy.",
    )
    parser.add_argument(
        "--no_press_policy",
        dest="no_press_policy",
        type=int,
        default=0,
        help="ü§êPolicy to use for no-press powers. Provide an integer to select a policy from no_press_policies.policy_map.",
    )
    parser.add_argument(
        "--super_exploiter_powers",
        dest="super_exploiter_powers",
        type=str,
        default="",
        help="ü§êComma-separated list of case-insensitive powers to use hybrid LM + RL exploiter policy.",
    )
    parser.add_argument(
        "--unit_threshold",
        dest="unit_threshold",
        type=int,
        default=10,
        help="ü§êNumber of enemy units on the board below which a super exploiter switches from the LLMAgent policy to the RL policy.",
    )
    parser.add_argument(
        "--center_threshold",
        dest="center_threshold",
        type=int,
        default=10,
        help="ü§êNumber of centers a super exploiter acquires before it switches back to the LLMAgent policy.",
    )

    args = parser.parse_args()
    return args


if __name__ == "__main__":
    try:
        main()
    except Exception as exc:
        # Manually write it with the logger so it doesn't get hidden in wandb logs
        tqdm.write("\n\n\n")  # Add some spacing
        exception_trace = "".join(
            traceback.TracebackException.from_exception(exc).format()
        )
        utils.log_error(
            logger,
            f"üíÄ FATAL EXCEPTION: {exception_trace}",
        )
        wandb.log(
            {
                "fatal_exception_trace": wandb.Table(
                    columns=["trace"], data=[[exception_trace]]
                )
            }
        )
        tqdm.write("\n\n\n")  # Add some spacing
        raise exc
